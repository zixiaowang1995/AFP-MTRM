{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer\n",
    "from dgllife.utils import AttentiveFPBondFeaturizer\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from dgllife.data import MoleculeCSVDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the Computational Device: GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('use GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('use CPU')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "torch.manual_seed(seed)            \n",
    "torch.cuda.manual_seed(seed)       \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and ProcessingÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_molgraphs(data):\n",
    "    assert len(data[0]) in [3, 4], \\\n",
    "        'Expect the tuple to be of length 3 or 4, got {:d}'.format(len(data[0]))\n",
    "    if len(data[0]) == 3:\n",
    "        smiles, graphs, labels = map(list, zip(*data))\n",
    "        masks = None\n",
    "    else:\n",
    "        smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    if masks is None:\n",
    "        masks = torch.ones(labels.shape)\n",
    "    else:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    return smiles, bg, labels, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings1 = pd.read_csv('train.csv')\n",
    "dc_listings2 = pd.read_csv('valid.csv')\n",
    "dc_listings3 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data,name,load):\n",
    "    dataset = MoleculeCSVDataset(data,\n",
    "                                 smiles_to_graph=smiles_to_bigraph,\n",
    "                                 node_featurizer=atom_featurizer,\n",
    "                                 edge_featurizer= bond_featurizer,\n",
    "                                 smiles_column='SMILES',\n",
    "                                 cache_file_path=str(name)+'_dataset.bin',\n",
    "                                 task_names=['pIC50_PIM1','pIC50_PIM2','pIC50_PIM3'],\n",
    "                                 load=load,init_mask=True,n_jobs=8\n",
    "                            )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = load_data(dc_listings1,'train',True)\n",
    "val_datasets = load_data(dc_listings2,'valid',True)\n",
    "test_datasets = load_data(dc_listings3,'test',True)\n",
    "train_loader = DataLoader(train_datasets, batch_size=20,shuffle=True,\n",
    "                          collate_fn=collate_molgraphs)\n",
    "vali_loader = DataLoader(val_datasets,batch_size=300,shuffle=True,\n",
    "                         collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(test_datasets,batch_size=300,shuffle=False,\n",
    "                         collate_fn=collate_molgraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Evaluation, and Prediction Functions for the AFP-MTRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(n_epochs, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    train_meter = Meter()\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        batch_data\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        bg=bg.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "        n_feats = bg.ndata.pop('hv').to(device)\n",
    "        e_feats = bg.edata.pop('he').to(device)\n",
    "        prediction = model(bg, n_feats, e_feats)\n",
    "        loss = (loss_criterion(prediction, labels) * (masks != 0).float()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meter.update(prediction, labels, masks)\n",
    "        losses.append(loss.data.item())\n",
    "    total_r2 = np.mean(train_meter.compute_metric('r2'))\n",
    "    total_loss = np.mean(losses)\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch {:d}/{:d}, training_r2 {:.4f}, training_loss {:.4f}'.format(epoch + 1, n_epochs, total_r2,total_loss))\n",
    "    return total_r2, total_loss\n",
    "\n",
    "def run_an_eval_epoch(n_epochs, model, data_loader,loss_criterion):\n",
    "    model.eval()\n",
    "    val_losses=[]\n",
    "    eval_meter = Meter()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            bg = bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "            n_feats = bg.ndata.pop('hv').to(device)\n",
    "            e_feats = bg.edata.pop('he').to(device)\n",
    "            vali_prediction = model(bg, n_feats, e_feats)\n",
    "            val_loss = (loss_criterion(vali_prediction, labels) * (masks != 0).float()).mean()\n",
    "            val_loss=val_loss.detach().cpu().numpy()\n",
    "            val_losses.append(val_loss)\n",
    "            eval_meter.update(vali_prediction, labels, masks)\n",
    "        total_score = np.mean(eval_meter.compute_metric('r2'))\n",
    "        total_loss = np.mean(val_losses)\n",
    "    return total_score, total_loss\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            bg = bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "            n_feats = bg.ndata.pop('hv').to(device)\n",
    "            e_feats = bg.edata.pop('he').to(device)\n",
    "            pred = model(bg, n_feats, e_feats)\n",
    "            preds.append(pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_zoo.AttentiveFPPredictor(node_feat_size=n_feats,\n",
    "                                   edge_feat_size=e_feats,\n",
    "                                   num_layers=2,\n",
    "                                   num_timesteps=1,\n",
    "                                   graph_feat_size=300,\n",
    "                                   n_tasks=3,\n",
    "                                   dropout=0.35\n",
    "                                    )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFP-MTRM model training and evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param ={}\n",
    "best_param[\"train_epoch\"] = 0\n",
    "best_param[\"valid_epoch\"] = 0\n",
    "best_param[\"train_MSE\"] = 9e8\n",
    "best_param[\"valid_MSE\"] = 9e8\n",
    "n_epochs = 500\n",
    "for e in range(n_epochs):\n",
    "    score, loss = run_a_train_epoch(n_epochs, e, model, train_loader, loss_fn, optimizer)\n",
    "    val_score, val_loss = run_an_eval_epoch(n_epochs, model, vali_loader,loss_fn)\n",
    "    if loss < best_param[\"train_MSE\"]:\n",
    "        best_param[\"train_epoch\"] = e\n",
    "        best_param[\"train_MSE\"] = loss\n",
    "    if val_loss < best_param[\"valid_MSE\"]:\n",
    "        best_param[\"valid_epoch\"] = e\n",
    "        best_param[\"valid_MSE\"] = val_loss\n",
    "        if val_loss < 0.6:\n",
    "             torch.save(model, str(e)+'.pt')\n",
    "    print(\"EPOCH:\\t\"+str(e)+'\\n'\\\n",
    "        +\"train_MSE\"+\":\"+str(loss)+'\\n'\\\n",
    "        +\"valid_MSE\"+\":\"+str(val_loss)+'\\n'\\\n",
    "         +\"train_R2\"+\":\"+str(score)+'\\n'\\\n",
    "         +\"val_R2\"+\":\"+str(val_score)+'\\n'\\\n",
    "        )\n",
    "    if (e - best_param[\"train_epoch\"] >18) and (e - best_param[\"valid_epoch\"] >35):        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Labels of Test Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(str(best_param[\"valid_epoch\"])+'.pt')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testpred.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in preds:\n",
    "        for value in row:\n",
    "            csvwriter.writerow([value])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
